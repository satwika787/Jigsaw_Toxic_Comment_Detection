# Jigsaw Toxic Comment Detection

## Overview
The Jigsaw Toxic Comment Detection project aims to identify and classify toxic comments from a large dataset. This project involves analyzing, pre-processing, and applying machine learning models to accurately detect and mitigate toxicity bias in comments.

## Project Description
### Data Analysis and Pre-processing
- **Dataset**: The dataset consists of over 100,000 comments, which was sourced from kaggle.
- **NLP Techniques**: Utilized Natural Language Processing (NLP) techniques including TF-IDF (Term Frequency-Inverse Document Frequency) and CountVectorizer to encode the text data.

### Model Implementation
- **Logistic Regression Classifier**: Incorporated a logistic regression classifier to mitigate toxicity bias, achieving a reliable prediction performance of 86.3%.
- **LSTM Architecture**: Trained and applied a Long Short-Term Memory (LSTM) network with word embeddings and a binary cross entropy loss function, resulting in an accuracy of 88.9%.

## Key Features
- Comprehensive data analysis and pre-processing for effective model training.
- Utilization of advanced NLP techniques to handle textual data.
- Implementation of robust machine learning models to detect toxic comments with high accuracy.

## Results
- **Logistic Regression Classifier**: Achieved a prediction performance of 86.3%.
- **LSTM Model**: Achieved an accuracy of 88.9%.

## Dataset
The dataset used in this project can be found on Kaggle: [https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge).

## Conclusion
This project demonstrates effective techniques for toxic comment detection using machine learning and NLP. The models developed showcase high accuracy and reliable performance, making significant contributions to mitigating toxicity in online platforms.
